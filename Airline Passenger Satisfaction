### IMPORTING PACKAGES AND DATASET

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns 
%matplotlib inline
import missingno as msno

#sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree  import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import VotingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import roc_auc_score, roc_curve
import scikitplot as skplt

import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter(action='ignore', category=FutureWarning)
pd.set_option('display.max_columns', None)
pd.options.display.float_format = '{:.3f}'.format




train = pd.read_csv('../input/airline-passenger-satisfaction/train.csv')
test = pd.read_csv('../input/airline-passenger-satisfaction/test.csv')
train = train.drop(['Unnamed: 0','id'],axis = 1)
test = test.drop(['Unnamed: 0','id'],axis = 1)


df = train.append(test)
df.head()


### EXPLORATORY DATA ANALYSIS


df.info()

df.describe([0.01, 0.05, 0.10, 0.80, 0.90, 0.95, 0.99]).T

print(df['satisfaction'].value_counts()), print(df['satisfaction'].value_counts() / df.shape[0])

df.duplicated().sum()

msno.matrix(df)

plt.figure(figsize=(20,12))
correlation_map = sns.heatmap(df.corr(), cmap="YlGnBu", annot=True)


### DATA VISUALIZATION

cat = ['Gender', 'Customer Type','Type of Travel','Class']

for i in cat:
    plt.figure(figsize = (14,8))
    sns.countplot(x = i, data = df, palette = 'bright')
    plt.show()
    
    
plt.figure(figsize = (14,8))
sns.countplot(x = 'satisfaction', data = df, palette = 'bright')
plt.show()


nums = df.select_dtypes(include=["int64","float64"])
nums.hist(bins = 14, layout = (4,5), figsize = (18,14),color = 'navy',grid = False)
plt.show()


### DATA PREPROCESSING

df.isna().sum()

df["Arrival Delay in Minutes"] = np.where(df["Arrival Delay in Minutes"].isnull(), df["Arrival Delay in Minutes"].mean(), df["Arrival Delay in Minutes"])
df.isna().sum()

df.info()

cat = train.select_dtypes(include=["object"]).columns
for col in df[cat].columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    

###  SPLITTING DATA AS TRAINING SET AND TEST SET

X = df.drop('satisfaction',axis = 1).values
y = df['satisfaction'].values

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state=41)


### MACHINE LEARNING MODELS

# Logistic Regression
log = LogisticRegression(solver = "liblinear")
log.fit(X_train, y_train)
y_pred_log = log.predict(X_test)

# SVM - Linear
svc = SVC(kernel = "linear", probability=True) 
svc.fit(X_train, y_train)
y_pred_svc = svc.predict(X_test)

# SVM - RBF
svc_rbf = SVC(kernel = "rbf",probability=True) 
svc_rbf.fit(X_train, y_train)
y_pred_svc_rbf = svc_rbf.predict(X_test)

# NAIVE BAYES
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)

# KNN
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

# DECISION TREE
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

# RANDOM FOREST
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# GBM
gbm = GradientBoostingClassifier()
gbm.fit(X_train, y_train)
y_pred_gbm = gbm.predict(X_test)

# BAGGING
bag = BaggingClassifier()
bag.fit(X_train, y_train)
y_pred_bag = bag.predict(X_test)

# ADABOOST
ada = AdaBoostClassifier()
ada.fit(X_train, y_train)
y_pred_ada = ada.predict(X_test)

# VOTING
clf1 = LogisticRegression(solver = "liblinear")
clf2 = RandomForestClassifier()
clf3 = KNeighborsClassifier()
vote = VotingClassifier(
    estimators=[('lr', clf1), ('rf', clf2), ('knn', clf3)], voting='hard')
vote.fit(X_train, y_train)
y_pred_vote = vote.predict(X_test)

# XGBOOST
xgb = XGBClassifier()
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

# LGBM
lgb = LGBMClassifier()
lgb.fit(X_train, y_train)
y_pred_lgb = lgb.predict(X_test)

# CATBOOST
cat = CatBoostClassifier()
cat.fit(train.drop("Survived",axis = 1), train.Survived, verbose = 0)
y_pred_cat = cat.predict(X_test)


### MODELS' PERFORMANCES

models = ["Logistic Regression","SVM-Linear","SVM-RBF","Naive Bayes","KNN","Decision Tree","Random Forest",
         "Gradient Boosting","Bagging","Adaboost","Voting","XGBoost","LGBM","Catboost"]

test_acc = [
    
    accuracy_score(y_test, y_pred_log),
    accuracy_score(y_test, y_pred_svc),
    accuracy_score(y_test, y_pred_svc_rbf),
    accuracy_score(y_test, y_pred_nb),
    accuracy_score(y_test, y_pred_knn),
    accuracy_score(y_test, y_pred_dt),
    accuracy_score(y_test, y_pred_rf),
    accuracy_score(y_test, y_pred_gbm),
    accuracy_score(y_test, y_pred_bag),
    accuracy_score(y_test, y_pred_ada),
    accuracy_score(y_test, y_pred_vote),
    accuracy_score(y_test, y_pred_xgb),
    accuracy_score(y_test, y_pred_lgb),
    accuracy_score(y_test, y_pred_cat),
    
]


train_acc = [
    
    accuracy_score(y_train, log.predict(X_train)),
    accuracy_score(y_train, svc.predict(X_train)),
    accuracy_score(y_train, svc_rbf.predict(X_train)),
    accuracy_score(y_train, nb.predict(X_train)),
    accuracy_score(y_train, knn.predict(X_train)),
    accuracy_score(y_train, dt.predict(X_train)),
    accuracy_score(y_train, rf.predict(X_train)),
    accuracy_score(y_train, gbm.predict(X_train)),
    accuracy_score(y_train, bag.predict(X_train)),
    accuracy_score(y_train, ada.predict(X_train)),
    accuracy_score(y_train, vote.predict(X_train)),
    accuracy_score(y_train, xgb.predict(X_train)),
    accuracy_score(y_train, lgb.predict(X_train)),
    accuracy_score(y_train, cat.predict(X_train))
    
]


pd.DataFrame({
    
    "Model":models,
    "Train Accuracy": train_acc,
    "Test Accuracy": test_acc
    
})














